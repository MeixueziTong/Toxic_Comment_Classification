{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification with PySpark\n",
    "\n",
    "## Author: Meixuezi Tong\n",
    "\n",
    "## Background\n",
    "Online Communications can be difficult due to the threat of abuse and harassment. In order to maintain a healthy online conversation environment, it is important for online communication platforms to build tools to automatically detect negative behaviors, for example, toxic comments (i.e. comments that are disrespectful or rude.) \n",
    "\n",
    "This study was aimed to implement PySpark pipeline tools to perform Natural Language Processing on raw text and fine-tune a Machine Learning model to classify toxic comments. The data used for this study was obtained and adapted from a public dataset from the Kaggle competition platform, provided by the Conversation AI research team. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Spark and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n",
      "Table VERSION_TRACKER created successfully\n",
      "Table METRICS_TRACKER created successfully\n",
      "\n",
      "Share anonymous install statistics? (opt-out instructions)\n",
      "\n",
      "PixieDust will record metadata on its environment the next time the package is installed or updated. The data is anonymized and aggregated to help plan for future releases, and records only the following values:\n",
      "\n",
      "{\n",
      "   \"data_sent\": currentDate,\n",
      "   \"runtime\": \"python\",\n",
      "   \"application_version\": currentPixiedustVersion,\n",
      "   \"space_id\": nonIdentifyingUniqueId,\n",
      "   \"config\": {\n",
      "       \"repository_id\": \"https://github.com/ibm-watson-data-lab/pixiedust\",\n",
      "       \"target_runtimes\": [\"Data Science Experience\"],\n",
      "       \"event_id\": \"web\",\n",
      "       \"event_organizer\": \"dev-journeys\"\n",
      "   }\n",
      "}\n",
      "You can opt out by calling pixiedust.optOut() in a new cell.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.9</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust runtime updated. Please restart kernel\n",
      "Table SPARK_PACKAGES created successfully\n",
      "Table USER_PREFERENCES created successfully\n",
      "Table service_connections created successfully\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.param import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql import *\n",
    "import pixiedust\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# setup SparkContext\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# drop unnecessary columns\n",
    "data = data.drop(['id','severe_toxic', 'obscene', 'threat', 'insult','identity_hate'], axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data examples\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "\n",
    "# Create fields and schema for creating dataframe from RDD\n",
    "fields = [StructField('text', StringType(), True), StructField('label', ByteType(), True) ]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(text=\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\", label=0)]\n"
     ]
    }
   ],
   "source": [
    "print(df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of toxic comments : 0.096: 15294 out of 159571\n"
     ]
    }
   ],
   "source": [
    "# calculate the ratio of toxic comments\n",
    "toxic = df.filter(df.label == 1).count()\n",
    "total_count = df.count()\n",
    "toxic_ratio = toxic/total_count\n",
    "print('The ratio of toxic comments : {:.3f}: {} out of {}'.format(toxic_ratio, toxic, total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Partition Training Dataset and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document count: 159571\n",
      "Training-set count: 127597\n",
      "Test-set count: 31974\n"
     ]
    }
   ],
   "source": [
    "# set 20% of the data for testing the model, 80% for training the model\n",
    "train_set, test_set = df.randomSplit([0.8, 0.2], 123)\n",
    "print (\"Total document count:\",df.count())\n",
    "print (\"Training-set count:\",train_set.count())\n",
    "print (\"Test-set count:\",test_set.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Construct a Pipeline for Feature Extractor and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules for feature transformation\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n",
    "\n",
    "#Constructing a pipeline for feature transformation, and model tuning\n",
    "\n",
    "#Tokenize into words\n",
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "#Remove stopwords\n",
    "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "\n",
    "#For each sentence (bag of words),use HashingTF to hash the sentence into a feature vector. \n",
    "hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "\n",
    "#Create TF_IDF features\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "\n",
    "# Create a Logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Streamline all above steps into a pipeline\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Perform a Parameter Grid Search to Fine Tune the Pipeline and Evaluate the Models by TrainValidationSplit Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the PR curve for best fitted model = 0.8748302691581958\n",
      "Accuracy 92.00% data items: 31974, correct: 29417\n"
     ]
    }
   ],
   "source": [
    "# Perform a grid search looking for the best parameters and the best models\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(hashingTF.numFeatures,[1000,5000,10000])\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.3, 0.6])\\\n",
    "    .build()\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=BinaryClassificationEvaluator().setMetricName('areaUnderPR'), # set area Under precision-recall curve as the evaluation metric\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train_set)\n",
    "# Make predictions\n",
    "train_prediction = model.transform(train_set)\n",
    "test_prediction = model.transform(test_set)\n",
    "\n",
    "print(\"Area under the PR curve for best fitted model =\",evaluator.evaluate(test_prediction))\n",
    "\n",
    "# Caculate the accuracy score for the best model \n",
    "correct = test_prediction.filter(test_prediction.label == test_prediction.prediction).count()  \n",
    "accuracy = correct/test_prediction.count()\n",
    "print('Accuracy {:.2%} data items: {}, correct: {}'.format(accuracy, test_prediction.count(), correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Output the Parameter Tuning Results in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dataframe to store model performance\n",
    "columns = ['sample_size','training_time/sec','testing_time/sec','areaUnderPR','acc_train','acc_test']\n",
    "index = [i for i in range(4)]\n",
    "pf = pd.DataFrame(index = index, columns = columns, dtype = 'float32') # transform the data from list format to pandas dataframe\n",
    "pf = pf.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>training_time/sec</th>\n",
       "      <th>testing_time/sec</th>\n",
       "      <th>areaUnderPR</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_size  training_time/sec  testing_time/sec  areaUnderPR  acc_train  \\\n",
       "0          0.0                0.0               0.0          0.0        0.0   \n",
       "1          0.0                0.0               0.0          0.0        0.0   \n",
       "2          0.0                0.0               0.0          0.0        0.0   \n",
       "3          0.0                0.0               0.0          0.0        0.0   \n",
       "\n",
       "   acc_test  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model  0\n",
      "training done in 190.30\n",
      "start predicting on test set\n",
      "testing done in 0.05\n",
      "accuracy on test data is 0.90\n",
      "accuracy on training data is 0.91\n",
      "start training model  1\n",
      "training done in 255.41\n",
      "start predicting on test set\n",
      "testing done in 0.04\n",
      "accuracy on test data is 0.90\n",
      "accuracy on training data is 0.90\n",
      "start training model  2\n",
      "training done in 382.68\n",
      "start predicting on test set\n",
      "testing done in 0.04\n",
      "accuracy on test data is 0.90\n",
      "accuracy on training data is 0.90\n",
      "start training model  3\n",
      "training done in 465.20\n",
      "start predicting on test set\n",
      "testing done in 0.04\n",
      "accuracy on test data is 0.92\n",
      "accuracy on training data is 0.94\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>training_time/sec</th>\n",
       "      <th>testing_time/sec</th>\n",
       "      <th>areaUnderPR</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>190.303772</td>\n",
       "      <td>0.054162</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.905323</td>\n",
       "      <td>0.903265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4</td>\n",
       "      <td>255.409302</td>\n",
       "      <td>0.043130</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.903602</td>\n",
       "      <td>0.903265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>382.681091</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.904582</td>\n",
       "      <td>0.903265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>465.204407</td>\n",
       "      <td>0.044251</td>\n",
       "      <td>0.87483</td>\n",
       "      <td>0.935829</td>\n",
       "      <td>0.920029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_size  training_time/sec  testing_time/sec  areaUnderPR  acc_train  \\\n",
       "0          0.2         190.303772          0.054162      0.50000   0.905323   \n",
       "1          0.4         255.409302          0.043130      0.50000   0.903602   \n",
       "2          0.8         382.681091          0.040246      0.50000   0.904582   \n",
       "3          1.0         465.204407          0.044251      0.87483   0.935829   \n",
       "\n",
       "   acc_test  \n",
       "0  0.903265  \n",
       "1  0.903265  \n",
       "2  0.903265  \n",
       "3  0.920029  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the training data into different sizes and evalutate the results\n",
    "\n",
    "from time import time # to compute training and testing time\n",
    "sample_range = [0.2, 0.4, 0.8, 1]\n",
    "for i in range(4):\n",
    "    \n",
    "    sample_size = sample_range[i]\n",
    "    \n",
    "    pf['sample_size'][i] = sample_size # store the sample size value\n",
    "    if sample_size == 1:\n",
    "        train = train_set # if the size is 1, no need to sample\n",
    "    else:\n",
    "        train = train_set.sample(False, sample_size, seed = 1234)\n",
    "        \n",
    "    print('start training model ', i)\n",
    "    # compute the training time\n",
    "    time1 = time()\n",
    "    model = tvs.fit(train)\n",
    "    time2 = time()\n",
    "    \n",
    "    pf['training_time/sec'][i] = time2 - time1 # store the training time value\n",
    "    print('training done in {:.2f}'.format(time2-time1))\n",
    "    train_prediction = model.transform(train)\n",
    "    \n",
    "    # compute the testing time\n",
    "    time3 = time()\n",
    "    print('start predicting on test set')\n",
    "    test_prediction = model.transform(test_set)\n",
    "    time4 = time()\n",
    "    pf['testing_time/sec'][i] = time4 - time3 # store the testing time value\n",
    "    print('testing done in {:.2f}'.format(time4-time3))\n",
    "    \n",
    "    # compute the Validation metric: areaUnderPR\n",
    "    pf['areaUnderPR'][i] = evaluator.evaluate(test_prediction)\n",
    "    \n",
    "    # compute the accuracy score on test data\n",
    "    cor_test = test_prediction.filter(test_prediction.label == test_prediction.prediction).count()  \n",
    "    pf['acc_test'][i] = cor_test/test_prediction.count() # store the test accuracy score \n",
    "    print('accuracy on test data is {:.2f}'.format(cor_test/test_prediction.count()))\n",
    "    \n",
    "    # compute the accuracy score on training data\n",
    "    cor_train = train_prediction.filter(train_prediction.label == train_prediction.prediction).count()  \n",
    "    pf['acc_train'][i] = cor_train/train_prediction.count() # store the training accuracy score\n",
    "    print('accuracy on training data is {:.2f}'.format(cor_train/train_prediction.count()))\n",
    "    \n",
    "\n",
    "pf       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pf.to_csv('performances.csv', index = False) # save the performance panda dataframe to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Output the best parameters for the best pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_44b99c0d73a3fd8b2275,\n",
       " StopWordsRemover_4592b912739472d2fcb3,\n",
       " HashingTF_457797ddf4311c34da7b,\n",
       " IDF_475e88e4a4bd8cef7c9c,\n",
       " LogisticRegression_4bedb56b0e71083e3c87]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bestModel.stages # check the pipleline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best HashingTF feature size:  10000\n",
      "best regularization parameter:  0.01\n",
      "best elastic net parameter:  0.0\n"
     ]
    }
   ],
   "source": [
    "# extract the best parameters from the best model\n",
    "best_model = model.bestModel\n",
    "print('best HashingTF feature size: ', best_model.stages[2]._java_obj.getNumFeatures())\n",
    "print('best regularization parameter: ', best_model.stages[4]._java_obj.getRegParam())\n",
    "print('best elastic net parameter: ', best_model.stages[4]._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3970055862471745,\n",
       " 0.5035470355479709,\n",
       " 0.544975240352693,\n",
       " 0.43630075265885787,\n",
       " 0.5179558877333595,\n",
       " 0.5489222540618783,\n",
       " 0.49677307181629665,\n",
       " 0.4975075361892045,\n",
       " 0.49786998286045986,\n",
       " 0.39466895690033155,\n",
       " 0.48509991860696533,\n",
       " 0.5223649551287207,\n",
       " 0.5477450484454648,\n",
       " 0.5477450484454648,\n",
       " 0.5477450484454648,\n",
       " 0.34620259759078076,\n",
       " 0.4281849520825269,\n",
       " 0.4569848303973037]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best parameters for the pipeline: , {Param(parent='LogisticRegression_4bedb56b0e71083e3c87', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_4bedb56b0e71083e3c87', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='HashingTF_457797ddf4311c34da7b', name='numFeatures', doc='number of features.'): 10000}\n"
     ]
    }
   ],
   "source": [
    "# zip the metric value with parameter values for all models\n",
    "grid_search_results = list(zip(model.validationMetrics, paramGrid))\n",
    "# sort the results by the metric value by descending order\n",
    "grid_search_results_sorted = sorted(grid_search_results, key=lambda x:x[0], reverse=True)\n",
    "print('the best parameters for the pipeline: ,', grid_search_results_sorted[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
